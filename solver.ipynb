{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a59235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aliso\\.conda\\envs\\alishir\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import InstanceNorm2d, LeakyReLU, Tanh, Sigmoid\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "import deeplay as dl\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f8c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2dfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_size=(256, 256)):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_size = image_size\n",
    "        self.rgb_transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "        ])\n",
    "        self.gray_transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,),(0.5,)),\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        gray = self.gray_transform(img)\n",
    "        rgb = self.rgb_transform(img)\n",
    "        return gray, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9b983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "class GrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads images from a directory (and subdirectories) as single-channel grayscale tensors.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Path to the directory containing images.\n",
    "        extensions (tuple of str): Allowed file extensions, e.g. ('.jpg', '.png').\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, extensions=('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        # Recursively collect image file paths\n",
    "        self.paths = sorted(\n",
    "            [p for p in self.root_dir.rglob('*') if p.suffix.lower() in extensions]\n",
    "        )\n",
    "        if not self.paths:\n",
    "            raise FileNotFoundError(f\"No images found in {root_dir} with extensions {extensions}\")\n",
    "\n",
    "        # Grayscale + ToTensor transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),  # outputs [1, H, W] in [0,1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        with Image.open(img_path) as img:\n",
    "            img = img.convert('RGB')  # ensure 3-channel input for Grayscale\n",
    "            clean = self.transform(img)\n",
    "        return clean\n",
    "\n",
    "\n",
    "class NoisyGrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that wraps GrayDataset and returns noisy and clean grayscale tensors.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Path to the directory containing images.\n",
    "        noise_std (float): Standard deviation of Gaussian noise added to clean image.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, noise_std=0.3, extensions=('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "        self.clean_ds = GrayDataset(root_dir, extensions)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean = self.clean_ds[idx]  # [1, H, W]\n",
    "        noise = torch.randn_like(clean) * self.noise_std\n",
    "        noisy = torch.clamp(clean + noise, 0.0, 1.0)\n",
    "        return noisy, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61382f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume your dataset classes are already defined: GrayDataset, NoisyGrayDataset\n",
    "\n",
    "import os\n",
    "import random\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def visualize_and_save_noisy_dataset(dataset, out_dir='output_images', max_images=10, seed=None):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Optional: set random seed for reproducibility\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Randomly select `max_images` indices from the dataset\n",
    "    total = len(dataset)\n",
    "    indices = random.sample(range(total), k=min(max_images, total))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        noisy, clean = dataset[idx]  # Directly index into dataset\n",
    "\n",
    "        # Save using torchvision\n",
    "        #save_image(clean, os.path.join(out_dir, f'gray_{i:03d}.png'), normalize=False)\n",
    "        save_image(noisy, os.path.join(out_dir, f'noisy_{i:03d}.png'), normalize=False)\n",
    "\n",
    "        # Optional: Also visualize inline with matplotlib\n",
    "        # fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "        # axs[0].imshow(clean.squeeze().cpu(), cmap='gray')\n",
    "        # axs[0].set_title('Clean')\n",
    "        # axs[0].axis('off')\n",
    "        # axs[1].imshow(noisy.squeeze().cpu(), cmap='gray')\n",
    "        # axs[1].set_title('Noisy')\n",
    "        # axs[1].axis('off')\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "root_dir = 'data/sub'  # Replace with actual path\n",
    "dataset = NoisyGrayDataset(root_dir, noise_std=0.05)\n",
    "visualize_and_save_noisy_dataset(dataset, out_dir='Noisy_gray', max_images=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47faf5c6",
   "metadata": {},
   "source": [
    "## jigsaw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c144d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def build_and_save_image(tiles: torch.Tensor,\n",
    "                         perm: torch.Tensor,\n",
    "                         grid_size: tuple[int, int],\n",
    "                         tile_size: int,\n",
    "                         save_path: str,\n",
    "                         fixed: False):\n",
    "    \"\"\"\n",
    "    Reconstructs a single image from shuffled tiles and saves it to disk.\n",
    "\n",
    "    Args:\n",
    "        tiles (Tensor[N, C, T, T]): Shuffled tiles (torch tensor).\n",
    "        perm  (Tensor[N]): Permutation such that tiles[k] came from original slot perm[k].\n",
    "        grid_size (H, W): Number of tiles vertically (H) and horizontally (W).\n",
    "        tile_size   (int): Width & height of each square tile in pixels.\n",
    "        save_path   (str): Path where the reconstructed image will be saved.\n",
    "    \"\"\"\n",
    "    H, W = grid_size\n",
    "    N = H * W\n",
    "    assert tiles.shape[0] == N, f\"Expected {N} tiles but got {tiles.shape[0]}\"\n",
    "    # Compute inverse permutation: inv_perm[original_idx] = position in `tiles`\n",
    "    if fixed:\n",
    "        inv_perm = perm.argsort()\n",
    "        # Reorder tiles back to their original scanning order\n",
    "        ordered_tiles = tiles[inv_perm]\n",
    "          # shape [N, C, T, T]\n",
    "    else:\n",
    "        # Reorder tiles back to their original scanning order\n",
    "        ordered_tiles = tiles[perm]\n",
    "          # shape [N, C, T, T]\n",
    "    # Convert each tensor tile to a PIL image\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    pil_tiles = [to_pil(t) for t in ordered_tiles.cpu()]\n",
    "\n",
    "    # Create a blank canvas\n",
    "    mode = pil_tiles[0].mode\n",
    "    canvas = Image.new(mode, (W * tile_size, H * tile_size))\n",
    "\n",
    "    # Paste each tile into its spot\n",
    "    for idx, tile in enumerate(pil_tiles):\n",
    "        i = idx // W  # row\n",
    "        j = idx % W   # col\n",
    "        canvas.paste(tile, (j * tile_size, i * tile_size))\n",
    "\n",
    "    # Save to disk\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    canvas.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc75f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3456 | Val Acc: 98.1720% | image_acc for val: 0.9386\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads images from a directory (and subdirectories) as single-channel grayscale tensors.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Path to the directory containing images.\n",
    "        extensions (tuple of str): Allowed file extensions, e.g. ('.jpg', '.png').\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, extensions=('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        # Recursively collect image file paths\n",
    "        self.paths = sorted(\n",
    "            [p for p in self.root_dir.rglob('*') if p.suffix.lower() in extensions]\n",
    "        )\n",
    "        if not self.paths:\n",
    "            raise FileNotFoundError(f\"No images found in {root_dir} with extensions {extensions}\")\n",
    "\n",
    "        # Grayscale + ToTensor transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),  # outputs [1, H, W] in [0,1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        with Image.open(img_path) as img:\n",
    "            img = img.convert('RGB')  # ensure 3-channel input for Grayscale\n",
    "            clean = self.transform(img)\n",
    "        return clean\n",
    "\n",
    "\n",
    "class NoisyGrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that wraps GrayDataset and returns noisy and clean grayscale tensors.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Path to the directory containing images.\n",
    "        noise_std (float): Standard deviation of Gaussian noise added to clean image.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, noise_std=0.3, extensions=('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "        self.clean_ds = GrayDataset(root_dir, extensions)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean = self.clean_ds[idx]  # [1, H, W]\n",
    "        noise = torch.randn_like(clean) * self.noise_std\n",
    "        noisy = torch.clamp(clean + noise, 0.0, 1.0)\n",
    "        return noisy, clean\n",
    "# Define transformation for tile extraction\n",
    "def get_tile_transform(tile_size, noise_std, augment=False):\n",
    "    transform_list = [\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),  # [1, H, W], range [0,1]\n",
    "    ]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "# Tile extraction as given\n",
    "def extract_tiles(img, grid_size, tile_size, overlap, augment, noise_std):\n",
    "    H, W = grid_size\n",
    "    w, h = img.size\n",
    "    cell_w, cell_h = w / W, h / H\n",
    "    transform = get_tile_transform(tile_size, noise_std, augment)\n",
    "    tiles = []\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            x0 = int(max(0, j*cell_w - overlap))\n",
    "            y0 = int(max(0, i*cell_h - overlap))\n",
    "            x1 = int(min(w, (j+1)*cell_w + overlap))\n",
    "            y1 = int(min(h, (i+1)*cell_h + overlap))\n",
    "            crop = img.crop((x0, y0, x1, y1)).resize((tile_size, tile_size), Image.BICUBIC)\n",
    "            tiles.append(transform(crop))\n",
    "    return torch.stack(tiles)  # shape: [H*W, 1, tile_size, tile_size]\n",
    "\n",
    "# Dataset that returns tiles with noise\n",
    "class TileNoisyGrayDataset(Dataset):\n",
    "    def __init__(self, root_dir, grid_size=(4, 4), tile_size=64, overlap=0, noise_std=0.3, augment=False):\n",
    "        self.base_dataset = GrayDataset(root_dir)\n",
    "        self.grid_size = grid_size\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "        self.noise_std = noise_std\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean_img = self.base_dataset[idx]  # [1, H, W]\n",
    "        img_pil = transforms.ToPILImage()(clean_img)\n",
    "\n",
    "        clean_tiles = extract_tiles(img_pil, self.grid_size, self.tile_size, self.overlap, self.augment, noise_std=0)\n",
    "        noise = torch.randn_like(clean_tiles) * self.noise_std\n",
    "        noisy_tiles = torch.clamp(clean_tiles + noise, 0.0, 1.0)\n",
    "\n",
    "        # Shuffle the tiles and get permutation\n",
    "        num_tiles = noisy_tiles.size(0)\n",
    "        perm = torch.randperm(num_tiles)\n",
    "        shuffled_noisy_tiles = noisy_tiles[perm]\n",
    "\n",
    "        return shuffled_noisy_tiles, perm\n",
    "\n",
    "    \n",
    "class LSCE(nn.Module):\n",
    "    def __init__(self, eps=0.05):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: [B*N, N]\n",
    "        target: [B*N]\n",
    "        \"\"\"\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        nll = -logp[torch.arange(target.size(0)), target]\n",
    "        smooth = -logp.mean(dim=1)\n",
    "        return ((1 - self.eps) * nll + self.eps * smooth).mean()\n",
    "    \n",
    "class JigsawModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size=(3, 3),\n",
    "        backbone='resnet50',\n",
    "        weights=ResNet50_Weights.IMAGENET1K_V2,\n",
    "        nhead=8,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        noise_std=0.05,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        H, W = grid_size\n",
    "        self.N = H * W\n",
    "\n",
    "        # --- Gaussian noise augmentation for noisy tiles ---\n",
    "        #self.noise = GaussianNoise(std=noise_std)\n",
    "\n",
    "        # --- CNN encoder (ResNet50 trunk w/o final FC) adapted for 1-channel input ---\n",
    "        base = getattr(models, backbone)(weights=weights)\n",
    "        # Replace first conv to accept single-channel input\n",
    "        orig_conv = base.conv1\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=orig_conv.out_channels,\n",
    "            kernel_size=orig_conv.kernel_size,\n",
    "            stride=orig_conv.stride,\n",
    "            padding=orig_conv.padding,\n",
    "            bias=(orig_conv.bias is not None)\n",
    "        )\n",
    "        # Initialize new conv weights by averaging across original RGB channels\n",
    "        with torch.no_grad():\n",
    "            new_conv.weight[:] = orig_conv.weight.mean(dim=1, keepdim=True)\n",
    "            if orig_conv.bias is not None:\n",
    "                new_conv.bias[:] = orig_conv.bias\n",
    "        base.conv1 = new_conv\n",
    "\n",
    "        # Remove final FC\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.embed_dim = base.fc.in_features\n",
    "\n",
    "        # --- Learnable positional embeddings, one per slot ---\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, self.N, self.embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_emb, std=0.02)\n",
    "\n",
    "        # --- Transformer Encoder over the sequence of tile embeddings ---\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Per-token MLP head to predict slot index 0…N-1 ---\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.embed_dim, self.N)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor[B, N, 1, T, T] (grayscale tiles)\n",
    "        returns logits: Tensor[B, N, N]\n",
    "        \"\"\"\n",
    "        B, N, C, T, _ = x.shape\n",
    "        # 1) Add noise to each tile\n",
    "        x = x.view(B * N, C, T, T).view(B, N, C, T, T)\n",
    "\n",
    "        # 2) CNN encode each tile → (B*N, D)\n",
    "        feats = self.encoder(x.view(B * N, C, T, T)).view(B, N, -1)\n",
    "\n",
    "        # 3) Add positional embedding\n",
    "        feats = feats + self.pos_emb\n",
    "\n",
    "        # 4) Contextualize with Transformer\n",
    "        feats = self.transformer(feats)\n",
    "\n",
    "        # 5) Predict a distribution over target slot for each input tile\n",
    "        logits = self.head(feats)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def hungarian_assign(self, logits, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Solve assignment per batch element via Hungarian on -logit scores.\n",
    "        Returns: Tensor[B, N] giving the assigned slot index for each tile.\n",
    "        \"\"\"\n",
    "        scores = (logits / temperature).cpu().numpy()\n",
    "        perms = []\n",
    "        for mat in scores:\n",
    "            _, col_ind = linear_sum_assignment(-mat)\n",
    "            perms.append(torch.tensor(col_ind, dtype=torch.long))\n",
    "        return torch.stack(perms, dim=0)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optim, scheduler, epoch, freeze_epochs=5):\n",
    "    model.train()\n",
    "    # freeze encoder if desired\n",
    "    for p in model.encoder.parameters():\n",
    "        p.requires_grad = (epoch > freeze_epochs)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_tiles = 0\n",
    "\n",
    "    for tiles, perm in tqdm(loader, desc=f\"Train Epoch {epoch}\"):\n",
    "        tiles, perm = tiles.to(device), perm.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        logits = model(tiles)                # [B, N, N]\n",
    "        B, N, _ = logits.shape\n",
    "\n",
    "        # --- loss ---\n",
    "        loss = LSCE()(logits.view(B*N, -1), perm.view(-1))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "\n",
    "        # --- accuracy ---\n",
    "        with torch.no_grad():\n",
    "            preds = model.hungarian_assign(logits)  # [B, N], on CPU\n",
    "            preds = preds.to(device)\n",
    "            running_correct += (preds == perm).sum().item()\n",
    "            running_tiles   += B * N\n",
    "\n",
    "        running_loss += loss.item() * B\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    accuracy = running_correct / running_tiles\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch_imagewise(model, loader, device):\n",
    "    model.eval()\n",
    "    criterion = LSCE()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tiles = correct_tiles = total_images = correct_images = 0\n",
    "    i = 0\n",
    "\n",
    "    for tiles, perm in loader:\n",
    "        tiles, perm = tiles.to(device), perm.to(device)\n",
    "        B, N = perm.shape\n",
    "\n",
    "        # Reconstruct and save the *first* image in the batch:\n",
    "        # build_and_save_image(\n",
    "        #     tiles[0].cpu(),     # [N, C, T, T]\n",
    "        #     perm[0].cpu(),      # [N]\n",
    "        #     grid_size=(3,3),\n",
    "        #     tile_size=96,\n",
    "        #     save_path=f'reconstructions/original_{i}.png',\n",
    "        #     fixed=False\n",
    "        # )\n",
    "\n",
    "        # Forward & loss\n",
    "        logits = model(tiles)\n",
    "        total_loss += criterion(\n",
    "            logits.view(B*N, -1),\n",
    "            perm.view(-1)\n",
    "        ).item() * B\n",
    "\n",
    "        # Hungarian assignment & save reconstruction\n",
    "        preds = model.hungarian_assign(logits).to(device)\n",
    "        # build_and_save_image(\n",
    "        #     tiles[0].cpu(),\n",
    "        #     preds[0].cpu(),\n",
    "        #     grid_size=(3,3),\n",
    "        #     tile_size=96,\n",
    "        #     save_path=f'reconstructions/pred_{i}.png',\n",
    "        #     fixed=True\n",
    "        # )\n",
    "\n",
    "        # Accuracy\n",
    "        eq = (preds == perm)\n",
    "        correct_tiles += eq.sum().item()\n",
    "        total_tiles   += B * N\n",
    "        correct_images += eq.all(dim=1).sum().item()\n",
    "        total_images   += B\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    tile_acc = correct_tiles / total_tiles\n",
    "    image_acc = correct_images / total_images\n",
    "\n",
    "    return avg_loss, tile_acc, image_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    epochs, bs = 300, 1\n",
    "    grid_size  = (3,3)\n",
    "    tile_size  = 96\n",
    "    overlap    = 0       # fixed\n",
    "    noise_std  = 0.1\n",
    "\n",
    "    # Model, optimizer, scheduler\n",
    "    model = JigsawModel(grid_size=grid_size, num_layers=2, nhead=8, dropout=0.1).to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load('checks/best_jigsaw.pth', map_location=device)\n",
    "    )\n",
    "\n",
    "    val_ds   = TileNoisyGrayDataset('data/test',   grid_size, tile_size, overlap, noise_std)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=bs, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    val_loss,   val_acc   , image_acc= validate_one_epoch_imagewise(model, val_dl, device)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4%} | \"\n",
    "    f\"image_acc for val: {image_acc:.4f}\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e4ca",
   "metadata": {},
   "source": [
    "## denoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774a8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_loss(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the average MSE loss of a model over a dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader returning (noisy, clean) image pairs.\n",
    "        device (torch.device): Device to perform computation on.\n",
    "\n",
    "    Returns:\n",
    "        float: Average MSE loss over the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean in data_loader:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "\n",
    "            output = model(noisy).clamp(0, 1)\n",
    "            loss = loss_fn(output, clean)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "    avg_loss = total_loss / count if count > 0 else float('nan')\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d615653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015966438404575456"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import deeplay as dl  # same library you used for UNet2d\n",
    "\n",
    "# 1. Device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Recreate the same Regressor + UNet template\n",
    "unet = dl.UNet2d(\n",
    "    in_channels=1,\n",
    "    channels=[16, 32, 65, 64, 128],\n",
    "    out_channels=1,\n",
    "    skip=dl.Cat(),\n",
    ")\n",
    "regressor = dl.Regressor(\n",
    "    model=unet,\n",
    "    loss=torch.nn.MSELoss(),\n",
    "    optimizer=dl.Adam(lr=1e-3),\n",
    ").create().to(device)\n",
    "\n",
    "# 2. Load the entire trained state\n",
    "regressor.load_state_dict(torch.load(\"denoiser_model_01.pth\", map_location=device))\n",
    "regressor.eval()\n",
    "\n",
    "# 3. The denoiser UNet is now at\n",
    "denoiser_net = regressor.model\n",
    "\n",
    "root_data_dir = 'data/test'\n",
    "test_set    = NoisyGrayDataset(root_data_dir, noise_std=0.1)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False,num_workers=0)\n",
    "\n",
    "evaluate_loss(denoiser_net,test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf666438",
   "metadata": {},
   "source": [
    "## color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93ea2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import InstanceNorm2d, LeakyReLU, Tanh, Sigmoid\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "import deeplay as dl\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "gen = dl.UNet2d(in_channels=1, channels=[16,32,64,128], out_channels=3)\n",
    "# Gradient checkpointing to save memory\n",
    "try:\n",
    "    gen.enable_gradient_checkpointing()\n",
    "except AttributeError:\n",
    "    pass\n",
    "# Norm + activations\n",
    "from torch.nn import LeakyReLU, Tanh\n",
    "gen['decoder','blocks',:-1].all.normalized(InstanceNorm2d)\n",
    "gen['decoder','blocks',:-1,'activation'].configure(LeakyReLU, negative_slope=0.2)\n",
    "gen['decoder','blocks',-1,'activation'].configure(Tanh)\n",
    "# Build\n",
    "gen.build().to(device)\n",
    "\n",
    "# Discriminator\n",
    "disc = dl.ConvolutionalNeuralNetwork(in_channels=4, hidden_channels=[8,16,32], out_channels=1)\n",
    "disc['blocks',...,'layer'].configure(kernel_size=4,stride=2,padding=1)\n",
    "disc['blocks',...,'activation#-1'].configure(LeakyReLU, negative_slope=0.2)\n",
    "disc['blocks',1:-1].all.normalized(InstanceNorm2d)\n",
    "disc['blocks',-1,'activation'].configure(Sigmoid)\n",
    "disc.build().to(device)\n",
    "\n",
    "# ------------------------\n",
    "# Losses & optimizers with mixed precision\n",
    "# ------------------------\n",
    "loss_disc = torch.nn.MSELoss()\n",
    "loss_recon = torch.nn.L1Loss()\n",
    "loss_percep = LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(device)\n",
    "optim_g = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "optim_d = torch.optim.Adam(disc.parameters(), lr=5e-5, betas=(0.5,0.999))\n",
    "scaler = torch.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "899ca593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generator_loss(model, loader, device, loss_recon=None, loss_percep=None):\n",
    "    model.eval()\n",
    "    recon_loss_fn = loss_recon or torch.nn.L1Loss()\n",
    "    total_recon = 0.0\n",
    "    total_percep = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        for gray, rgb in loader:\n",
    "            gray, rgb = gray.to(device), rgb.to(device)\n",
    "            fake_rgb = model(gray).clamp(-1, 1)\n",
    "\n",
    "            recon = recon_loss_fn(fake_rgb, rgb)\n",
    "            total_recon += recon.item()\n",
    "\n",
    "            if loss_percep is not None:\n",
    "                perc = loss_percep(fake_rgb, rgb)\n",
    "                total_percep += perc.item()\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    avg_recon = total_recon / count\n",
    "    avg_percep = total_percep / count if loss_percep is not None else None\n",
    "\n",
    "    return avg_recon, avg_percep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliso\\AppData\\Local\\Temp\\ipykernel_31756\\3101141143.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average L1 Loss: 0.0861\n",
      "Average LPIPS Loss: 0.1755\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# 0) Load model\n",
    "ckpt_path = 'color.pth'\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gen.to(device)\n",
    "gen.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "gen.eval()\n",
    "\n",
    "test_paths  = glob('data/test/*.png')\n",
    "\n",
    "img_size=(256,256)\n",
    "batch_size=16\n",
    "test_ds = ColorizationDataset(test_paths, image_size=img_size)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, pin_memory=True,  num_workers=0)\n",
    "\n",
    "percep_loss = LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(device)\n",
    "\n",
    "# Evaluate\n",
    "avg_l1, avg_percep = evaluate_generator_loss(gen, test_loader, device, loss_recon=torch.nn.L1Loss(), loss_percep=percep_loss)\n",
    "\n",
    "print(f\"Average L1 Loss: {avg_l1:.4f}\")\n",
    "if avg_percep is not None:\n",
    "    print(f\"Average LPIPS Loss: {avg_percep:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
